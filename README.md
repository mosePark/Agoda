## Does a Large Language Model Really Speak in Human-Like Language?

[[Arxiv Version](https://arxiv.org/abs/2501.01273)]  
[[Published Version](https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.70060)]

---

## 🔍 Overview

Despite the growing fluency of large language models (LLMs), there is still limited understanding of how “human-like” their language actually is.  
This study examines whether LLMs produce text that **structurally resembles human-written language** — not just on the surface, but in deeper patterns such as discourse structure and word usage communities.

We develop a statistical framework to evaluate these differences by comparing:

- **Original human-written reviews**
- **Paraphrased versions generated by GPT-3.5**
- **Paraphrased outputs that were passed through the model twice**

The analysis is based on textual data embedding and structural comparisons of writing style and variation patterns.

---

## 🔬 What We Did

- Collected a large dataset of customer reviews written by humans.
- Used GPT-3.5 to paraphrase these reviews once, then again.
- Compared the patterns and structures among the original, once-paraphrased, and twice-paraphrased texts.
- Focused on **community structure** — groupings of similar expressions and themes — within each corpus.
- Measured the **distance between these groupings** to assess whether LLMs are simply repeating patterns or truly mimicking human variation.

---

## 💡 Core Idea

This study is built around two central questions to probe the human-likeness of LLM-generated text:

1. **Structural Symmetry**: If LLM-generated text truly mirrors human language, then the difference between human and once-paraphrased text should be similar to that between once- and twice-paraphrased text. This assumption relies on a paired data structure where each sample has aligned paraphrases.

2. **Effect of Text Variability**: LLMs allow control over output diversity through temperature settings. We explore how varying this parameter affects the structural closeness between human and machine-generated texts.

To address these questions, the authors introduce a statistical hypothesis testing framework that compares latent community structures in embedding space. The key idea is to assess whether paraphrased texts retain the same grouping patterns as their human-written counterparts when viewed through a shared reference space.

---

## 🧠 Key Insights

- LLM-generated texts are **fluent but structurally different** from human writing.
- Even small shifts in diversity parameters (like temperature) don't eliminate this difference — they only reduce it slightly.
- Paraphrasing LLM-generated text repeatedly does not lead to human-like results. Instead, it tends to reinforce non-human-like structures.
- This suggests that LLMs do not fully replicate the nuanced, organic variability of human language.

---

## 📁 Datasets Used

- **Human-written reviews** (originals)
- **GPT-3.5 paraphrased reviews**
- **Double-paraphrased outputs** (GPT → GPT)
- Additional benchmark corpora like CNN/DailyMail, SQuAD2, and Quora used for robustness checks

---

## 🧾 Citation

```bibtex
@article{park2025does,
  title={Does a Large Language Model Really Speak in Human-Like Language?},
  author={Park, Mose and Choi, Yunjin and Jeon, Jong-June},
  journal={Stat},
  volume={14},
  number={2},
  pages={e70060},
  year={2025},
  publisher={Wiley Online Library}
}
